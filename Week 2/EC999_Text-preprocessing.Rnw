\documentclass{beamer}
\usetheme{default}
%\usetheme{Malmoe}

\title[EC999: Quantitative Text Analysis]{EC999: Text Normalization} \def\newblock{\hskip .11em plus .33em minus .07em}


\def\Tiny{\fontsize{10pt}{10pt}\selectfont}
\def\smaller{\fontsize{8pt}{8pt}\selectfont}

\institute[Warwick]{University of Chicago \& University of Warwick}
\author[Thiemo Fetzer]{Thiemo Fetzer}

 \date{\today}

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{graphics}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{natbib}
\usepackage{hyperref}
%\usepackage{enumitem}
 \usepackage{pgffor}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}
\usepackage{verbatim} 
\usepackage{cancel}
\newcommand\xxcancel[1]{\xcancel{#1}\vphantom{#1}}

\usepackage{mathtools,xparse}

\newenvironment{Description}
               {\list{}{\labelwidth=0pt \itemindent-\leftmargin
                        \let\makelabel\Descriptionlabel
                        % or whatever
               }}
               {\endlist}
\newcommand*\Descriptionlabel[1]{%
  \hspace\labelsep
  \normalfont%  reset current font setting
  \color{blue}\bfseries\sffamily% or whatever 
  #1}


\setbeamersize{text margin left = 16pt, text margin right = 16pt}
\newcommand{\code}[1]{\texttt{#1}}

\newenvironment<>{algorithm}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Algorithm}%
\else%
   \def\insertblocktitle{Algorithm ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=yellow!50!black}
  \setbeamercolor{block body}{fg=black,bg=yellow!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}


\newenvironment<>{assumption}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Assumption}%
\else%
   \def\insertblocktitle{Assumption ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=blue!50!black}
  \setbeamercolor{block body}{fg=black,bg=blue!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}

%changing spacing between knitr code and output
\usepackage{etoolbox} 
\makeatletter 
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt } 
\makeatother
\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(data.table)
library(calibrate)
library(plyr)
opts_chunk$set(fig.path='figures/knitr-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
options(scipen = 1, digits = 3)
options(warn=-1)
setwd("~/Dropbox/Teaching/QTA/Lectures/Week 2")
options(stringsAsFactors = FALSE)
library(glmnet)
library(ggplot2)
library(data.table) 
library(RJSONIO)
library(quanteda)

@

\AtBeginSection[]
{
 \begin{frame}<beamer>
 \frametitle{Plan}
 \tableofcontents[currentsection]
 \end{frame}
}
\maketitle
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Quantitative Text Analysis as process}
Above all, there needs to be a formulatedresearch question or \textbf{goal} to be achieved.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=.4]{figures/qta-steps}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Building A Corpus}

\begin{itemize}

\item Textual data can be stored in multiple different formats

\begin{itemize}
\item JSON (JavaScript Object Notation) is a lightweight data-interchange format for structured data.
\item XML 
\item Flat text files
\item (machine readable?) PDFs
\end{itemize}

\item Parsing or reading text data into R can be achieved by a range of functions.
\end{itemize}

<<readingtext, size="tiny",eval=FALSE, tidy=TRUE>>=
#con can be any connection, could be a URL or a path to a file
TEXT<-readLines(con="https://www.dropbox.com/s/eynnvac4kurnjon/speaches-2016-election.json?dl=1", encoding = "UTF-8")
#commonly used for tabular data formats
TEXT<-read.table(file="file.txt")
#may need to iterate over a whole folder of documents
TEST<-lapply(list.files("Path"), function(x) readLines(con=x))
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Congressional speeches}

\begin{itemize}

\item Loading Congressional speaches by important figures important to the 2016 presidential election.
\item Data is JSON format
\item Each line is a speach given by a member of congress.
\item JSON data provides string excerpt as well as meta-information: date, party, speaker, chamber,...
\end{itemize}
\begin{figure}[h]
\begin{center}
\includegraphics[scale=.33]<1>{figures/json-speaches}
\end{center}
%\caption{\small{EU Enlargement in 2004}}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Congressional speaches}
<<readingspeeches, size="tiny",eval=TRUE,cache=TRUE, tidy=TRUE>>=
options(stringsAsFactors=FALSE)
library(data.table) 
library(RJSONIO)
library(quanteda)
TEXT<-readLines(con="https://www.dropbox.com/s/eynnvac4kurnjon/speaches-2016-election.json?dl=1")
TEXT[1]
SPEECHES<-lapply(TEXT, function(x) data.frame(fromJSON(x)))
SPEECHES<-rbindlist(SPEECHES)
SPEECHES[1]
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: A Corpus of Congressional speaches}
<<corpusspeeches, size="tiny",eval=TRUE,cache=TRUE, tidy=TRUE>>=

CORPUS<-corpus(SPEECHES$text) 
CORPUS[["congress"]]<-SPEECHES$congress
CORPUS[["speaker_name"]]<-SPEECHES$speaker_name
CORPUS[["speaker_party"]]<-SPEECHES$speaker_party
CORPUS[["date"]]<-SPEECHES$date
summary(CORPUS, n=10)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Fundamentals about text data}

There are very few ``fundamental law's'' in computational linguistic.  The exception are \emph{Heap's Law} and \emph{Zipf's Law}, which highlights why most text data is \emph{sparse}. 

Typically we will define a model of language that is a \emph{stochastic} process. 

\begin{itemize}

\item Study the single occurence of a word, not its frequency - \emph{Bernoulli process}

\item Modeling word frequencies: \emph{Poisson} or \emph{multionomial} distribution.

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Heap's Law}
\emph{Heaps' law} (also called Herdan's law) is an empirical relationship which describes the number of \emph{distinct words} in a document (or set of documents) as a function of the \emph{document length} (so called type-token relation). It can be formulated as
$$|V| = k N^{\beta}$$
In log-log form, this power law becomes a straight line

$$\log(|V|) = k + \beta \log(N)$$

where $|V|$ is the size of the vocabulary (the number of types) and $N$ is the number of tokens.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<backupsotuillus, echo=FALSE, include=FALSE, eval=TRUE, chache=TRUE, tidy=TRUE, size="tiny">>=
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
DF<-summary(SOTUCorpus)
plot(log(DF$Types),log(DF$Tokens))+abline(lm(log(DF$Tokens) ~ log(DF$Types)))

@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Illustration of Heap's Law in State of Union speeches}
<<sotuheapsillustration, echo=TRUE, include=TRUE, eval=FALSE, chache=TRUE, tidy=TRUE, size="tiny">>=
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
DF<-summary(SOTUCorpus)
plot(log(DF$Types),log(DF$Tokens))+abline(lm(log(DF$Tokens) ~ log(DF$Types)))
@

\begin{center}
\includegraphics[scale=0.4]{figures/knitr-backupsotuillus-1.pdf}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Illustration of Heap's Law in State of Union speeches}

<<sotuheapsillustration2, echo=TRUE, include=TRUE, eval=TRUE, chache=TRUE, tidy=TRUE, size="tiny">>=
summary(lm(log(DF$Tokens) ~ log(DF$Types)))
@

For larger corpora, the coefficient is typically smaller. Stemming and further tokenization typically lowers the vocabulary space.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Zipf's Law}
Zipf's Law is a law about the frequency distribution of words \emph{within a document}. 

Zipf's Law states that s frequency of any word is inversely proportional to its rank in the frequency table.\\

Formally: Word frequency

$$f = \frac{a}{r^b}$$

where $r$ is the rank in the (empirical) word frequency distribution. Again, logging

$$\log(f) = \log(a) - b \log(r)$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Illustration of Zipf's Law}
<<zipfsillustration, echo=TRUE, include=TRUE, eval=TRUE, chache=TRUE, tidy=TRUE, size="tiny">>=
OBAMA<-subset(SOTUCorpus, filename=="su2012.txt")
TOK<-tokenize(OBAMA, removePunct=TRUE)
TOK<-data.table(token=tolower(unlist(TOK)))
TOK<-TOK[, .N, by=token][order(N, decreasing=TRUE)]
TOK[1:20]
TOK[,rank :=1:nrow(TOK)]
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Illustration of Zipf's Law}
<<zipfsillustration2, echo=TRUE, include=TRUE, eval=TRUE, chache=TRUE, tidy=TRUE, size="tiny",out.width='3in'>>=
plot(log(TOK$N),log(TOK$rank))+abline(lm(log(TOK$N) ~ log(TOK$rank)))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Illustration of Zipf's Law}
<<zipfsillustration3, echo=TRUE, include=TRUE, eval=TRUE, chache=TRUE, tidy=TRUE, size="tiny",out.width='3in'>>=
summary(lm(log(TOK$N) ~ log(TOK$rank)))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Implications of Heap's and Zipf's Law}

\begin{itemize}

\item \emph{Heap's Law} and \emph{Zipf's Law} imply that data matrices constructed from text data is very \emph{sparse}.

\item Sparsity implies that there would be many zeroes.

\item Most data processing steps for text data involve \emph{densifying} the word frequency distribution. 

\item We next discuss a range of steps commonly used to densify.
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Word Tokenization and Normalization}

\begin{itemize}

\item \textbf{Tokenization} - task of segmenting running text into words.
\begin{itemize}

\item Plain vanilla approaches would just \code{str\_split(text," ")} - splitting by white spaces. 

\item More sophisticated methods apply \emph{locale} (language) specific algorithms.

\end{itemize}

\item \textbf{Normalization}- task of putting words/tokens into a standardized format.

\begin{itemize}
\item For example \code{we're} to \code{we are}.

\item Casefolding of tokens (lower-case or upper case)

\end{itemize}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{quanteda} tokenization routine}
We have already used the functionality in a few illustrations, but lets systematically introduce it here.

<<tokenizefunctionhelp, tidy=TRUE, size="tiny", eval=FALSE>>=
tokenize(x, what = c("word", "sentence", "character",
  "fastestword", "fasterword"), removeNumbers = FALSE, removePunct = FALSE,
  removeSymbols = FALSE, removeSeparators = TRUE, removeTwitter = FALSE,
  removeHyphens = FALSE, removeURL = FALSE, ngrams = 1L, skip = 0L,
  concatenator = "_", simplify = FALSE, verbose = FALSE, ...)
@

Tokenization function allows separation of words, sentences and individual characters from a character vector $x$ or a corpus object.  
\begin{center}
\includegraphics[scale=0.4]{figures/tokenizer-options.png}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{quanteda} tokenization routine}
\begin{itemize}

\item Dumb tokenization approach works reasonably well for languages based on latin alphabet. 
\item You may end up tokenizing features that you do not really want to separate, like \emph{Named Entities} - \code{New York} (next week we will work on detecting such n-grams)

\item Works poorly for languages that do not use white space character for separation (e.g. Chinese)

\item \code{word} option uses the \code{BreakIterator} algorithm that implements the \emph{Unicode Text Segmentation} standard

\item Words boundaries are identified according to the rules in \url{http://www.unicode.org/reports/tr29/#Word_Boundaries}, supplemented by a word dictionary for text in Chinese, Japanese, Thai or Khmer. The rules used for locating word breaks take into account the alphabets and conventions used by different languages.

\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{quanteda} tokenization routine}
Lets look at impact of the three alternative word tokenization methods for Obama's speeches.
<<tokenizefunctionmethods, tidy=TRUE, size="tiny", eval=TRUE, include=TRUE,echo=TRUE,fig.show="hide",results="hide">>=
for(i in c("word", "fastestword", "fasterword")) { 
  TOK<-data.table(tok=unlist(tokenize(OBAMA, what=i, removePunct=TRUE)))[ ,.N, by=tok][order(N, decreasing=TRUE)]
  TOK[, rid := 1:nrow(TOK)]
plot(hist(log(TOK$N)), main=i) 
}
@
\begin{figure}[h!]
\begin{center}$
\begin{array}{ccc}
 \includegraphics[scale=.2]{figures/knitr-tokenizefunctionmethods-2.pdf}&  \includegraphics[scale=.2]{figures/knitr-tokenizefunctionmethods-4.pdf}& \includegraphics[scale=.2]{figures/knitr-tokenizefunctionmethods-6.pdf}
\end{array}$
\end{center}
\end{figure}
We \emph{may} want to shift more mass to the right (higher counts). Little effect of densification of distribution of token counts using different methods.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{quanteda} tokenization routine}
\begin{itemize}
\item Depending on the application or which the data is prepared, it may make sense to normalize text by lowercaseing it, removing punctuation (as we have done already).

\item This may introduce \emph{noise} or inaccuracies, but its important to bear in mind what is the goal of the application.

\item in $R$, lowercasing is achieved with the function \code{tolower()}.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Lemmatization and Stemming}

Sparsity is a central issue as it blows up the underlying data matrices we work with.  There are a range of methods to select features and densify resulting data matrices.

\begin{Description}

\item[document frequency] cutoffs around how many documents does a term appear. 

\item[term frequency] cutoffs around how often a term appears in a corpus

\item[lemmatization] densification based on identified linguistic roots, disregarding the underlying parts of speech (verbs and adjective)

\item[deliberate disregard] exclude a range of stop words: words that do not provide independent substantive content

\item[purposive selection] use of dictionaries of words or phrases, possible identified from the underlying data (like collocations) or identified as having ``predictive content'' along dimension of interest.

\item[declared equivalency class] work of synonyms and map word (stems) to their underlying synonym

\end{Description}

We will discuss these in the next set of slides...
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Lemmatization and Stemming}

Lemmatization is the task of determining that two words have the same linguisting root.

\begin{itemize}

\item \code{am, are, is} have the same root being \code{be}

\item Plural's for nouns, in English usually identified by an added \code{s} share the same root.

\item Other gramatic constructs, like \emph{superlatives}...

\end{itemize}

The most common approach for English is to work with the \emph{Porter stemmer}, which simply chops off affixes. More complex methods use look up tables or augment process with information on the Part of Speech.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Porter Stemmer}

\begin{itemize}

\item Algorithm dates from 1980
\item Still the default ``go-to'' stemmer as it provides a good  trade-off between speed, readability, and accuracy

\item Stems using a set of rules, or transformations, applied in a succession of steps

\item In total there are about 60 rules in 6 steps that are applied iteratively
\end{itemize}

The sequence of steps can be summarized as follows:
\begin{enumerate}
\item Get rid of plurals and \code{-ed} or \code{-ing} suffixes
\item Turns terminal \code{y} to \code{i} when there is another vowel in the stem

\item Maps double suffixes to single ones: \code{-ization}, \code{-ational}, etc.

\item Deals with suffixes, \code{-full}, \code{-ness} etc.

\item Takes off \code{-ant}, \code{-ence}, etc.

\item Removes a final \code{-e} 

\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Porter Stemmer Examples}

\begin{enumerate}
\item Get rid of plurals and \code{-ed} or \code{-ing} suffixes
\item Turns terminal \code{y} to \code{i} when there is another vowel in the stem

\item Maps double suffixes to single ones: \code{-ization}, \code{-ational}, etc.

\item Deals with suffixes, \code{-full}, \code{-ness} etc.

\item Takes off \code{-ant}, \code{-ence}, etc.

\item Removes a final \code{-e} 

\end{enumerate}

Semantically $\rightarrow$ semantically $\rightarrow$ semanticalli $\rightarrow$ semantical  $\rightarrow$ semantic $\rightarrow$ semant $\rightarrow$ semant. \smallskip


Destructiveness $\rightarrow$ destructiveness $\rightarrow$ destructiveness $\rightarrow$ destructive $\rightarrow$ destructive $\rightarrow$ destruct $\rightarrow$ destruct\smallskip

Recognizing $\rightarrow$ recognize $\rightarrow$ recognize $\rightarrow$ recognize $\rightarrow$ recognize $\rightarrow$ recognize $\rightarrow$ recognize

Online illustration: \url{http://9ol.es/porter_js_demo.html}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{$R$ implementation}

Most implementaiton of Porter stemmer used in $R$ are actually coded in $C$, as $C++$ is much faster in processing.

<<snowball, tidy=TRUE, size="tiny", eval=TRUE, include=TRUE,echo=TRUE,fig.show="hide">>=
library("SnowballC")

wordStem("Amazing")

#multiple languages are supported
getStemLanguages()

wordStem("Liebschaften", language="de")
wordStem("amaren", language="es")

#densification?
TOK[, stemmed := wordStem(tok)]
nrow(TOK)
summary(TOK$N)
STEM<-TOK[, sum(N),by=stemmed]
plot(hist(log(STEM$V1)))
@


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Stemming reduces dimensionality}

\begin{center}
\includegraphics[scale=0.38]{figures/knitr-snowball-1.pdf}
\end{center}

Mass is shifted to the right, away from words occuring just once.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Stemming Issues}

Stemming is an approximation to the Lemmatization task which generally provides for a good trade-off between accuracy and speed. Most are stimple rule based algorithms.

\begin{itemize}

\item Stemmers are rudimentary approach to morphological analysis

\item No word sense disambiguation (``Police`` vs ``policing``)

\item No Part of Speech disambiguation (``Policing`` could be noun or verb, but ``hitting`` could only be verb)

\item However other approaches to lemmatization in practice does not do
much better. 

\end{itemize}

We just briefly introduce an alternative $R$ package that implements a morphological approach.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{hunspell} package}

\begin{itemize}

\item \code{hunspell} is actually the spell checker used in Google Chrome, which is also used by other proprietary software packages.

\item Has a significant capacities to identify lemmas of words using a dictionary lookup approach.

\end{itemize}

<<hunspell, tidy=TRUE, size="tiny", eval=TRUE, warning=FALSE,message=FALSE, include=TRUE,echo=TRUE,fig.show="hide">>=
words <- c("severing", "several", "ironic", "iron", "animal", "animated")
wordStem(words)
library(hunspell)
#hunspell_stem(words)
hunspell_analyze(words)
@



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{\code{stopwords}}

Stopwords are words that typically contain no informational content, they may be articles, prepositions, ...

<<stopwords, tidy=TRUE, size="tiny", eval=TRUE, warning=FALSE,message=FALSE, include=TRUE,echo=TRUE,fig.show="hide">>=
stopwords("english")[1:20]

stopwords("spanish")[1:20]

stopwords("german")[1:20]
@

Identifying words that can be removed as they are stopwords may use statistical methods, such as corpus dissimilarity, which we will introduce in the collocation detection lecture this week.

In \code{quanteda} you can remove features from a \code{tokenize}-object by applying the \code{removeFeatures(x, features)}, where \code{features} could be \code{stopwords("english")}.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Wordnet based densification}
\begin{quote}
WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. 
\end{quote}

\begin{itemize}

\item Need to separately install wordnet, in Mac can be done quickly using homebrew. 
\code{brew install wordnet}

\item R-package called \code{wordnet}

\item On loading, need to set path to wordnet dictionary installation.

\item Available to browse on \url{http://wordnetweb.princeton.edu/perl/webwn?s=car}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Wordnet based densification}
<<wordnet,tidy=TRUE, size="tiny", eval=TRUE, include=TRUE,echo=TRUE,fig.show="hide">>=
library(wordnet)
#set path to dictionary
setDict("/usr/local/Cellar/wordnet/3.1/dict")
synonyms("company", "NOUN")
@
Could list word list (running part of speech tagging first) and then replace synonyms of most frequently appearing words to reduce the vocabulary.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Minimum Edit Distances}

A lot of NLP work consists of identifying which texts are similar to others. We will illustrate this later, when we turn to a \emph{bag of words} language model that allows simple \emph{vector based} comparisons of text.\smallskip

We introduce the idea of computing string similarity introducing the idea of Edit Distance.\smallskip

Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other.
I will show an application from my research.\smallskip

This is extremely useful when working with (messy) data - such as OCRed documents, where you need to get standardize and get rid of non-systematic typos. \smallskip


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Levenshtein Distance}

\begin{itemize}

\item Levenshtein distance assumes a cost of deletion/ insertion of a character to be 1.

\item Assumes a cost of substition of character of 1 (sometimes 2).

\item So the Levenshtein distance between \code{car} and \code{can} is equal to 1.

\item Unit cost allows express adjustments needed relative to string length.

\item Levenshtein computation uses \textbf{dynamic programming} and is thus very fast.

\begin{quote}
dynamic programming (also known as dynamic optimization) is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems
\end{quote}

\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Minimum Edit Distances}
Suppose you have two strings $s$ and $t$ of length $n$ and $m$.

Below provides the algorithm
\begin{table}
\scalebox{0.7}{
\begin{tabular}{ll}
\toprule
Step & Description \\
\midrule
1 & Set n to be the length of a. \\
 & Set m to be the length of b. \\
 & If $n = 0$, return m and exit. \\
 & If $m = 0$, return n and exit. \\
 & Construct a matrix containing $0..m$ rows and $0..n$ columns. \\
2 & Initialize the first row to $0..n$. \\
 & Initialize the first column to 0..m. \\
3 & Examine each character of a (i from 1 to n). \\
4 & Examine each character of b (j from 1 to m). \\
5 & If $a[i]$ equals $b[j]$, the cost is 0. \\
 & If $a[i]$ doesn't equal $b[j]$, the cost is 1. \\
6 & Set cell d[i,j] of the matrix equal to the minimum of: \\
 & a. The cell immediately above plus 1: $d[i-1,j] + 1$. \\
 & b. The cell immediately to the left plus 1: $d[i,j-1] + 1$. \\
 & c. The cell diagonally above and to the left plus the cost: $d[i-1,j-1] + cost$. \\
7 & After the iteration steps (3, 4, 5, 6) are complete, the distance is found in cell $d[n,m]$. \\

\bottomrule
\end{tabular}
}
\end{table}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Levenshtein Distance}

Formally, Levenshtein Distance is computed as

$$ \qquad\operatorname{lev}_{a,b}(i,j) = min \begin{cases}
          \operatorname{lev}_{a,b}(i-1,j) + 1 \\
          \operatorname{lev}_{a,b}(i,j-1) + 1 \\
          \operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} $$
       
       
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Illustration}
\includegraphics[scale=.35]{figures/lev-computation.png}

\textbf{Initialization} Step 1 and 2

\begin{table}
\begin{center}
\begin{tabular}{llllllllllll}
\toprule
 &  & e & x & e & c & u & t & i & o & n \\
\midrule
 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
i & 1 \\
n & 2  \\
t & 3 \\
e & 4 \\
n & 5 \\
t & 6 \\
i & 7 \\
o & 8 \\
n & 9 &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Illustration}
\textbf{Step 3} for each row , for each column ...
\begin{table}
\begin{center}
\begin{tabular}{llllllllllll}
\toprule
 &  & e & x & e & c & u & t & i & o & n \\
\midrule
 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
i & 1 & \textbf{1} & 2 & 3 & 4 & 5 & 6 & 6 & 7 & 8 \\
n & 2  \\
t & 3 \\
e & 4 \\
n & 5 \\
t & 6 \\
i & 7 \\
o & 8 \\
n & 9 &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Converting "e" to "i": min of 

\begin{itemize}
\item Converting "empty string" to "i", plus deletion (left cell)
\item Converting "e" to "empty string", plus insertion (upper cell)
\item Converting "empty" to "empty", plus substitution of "e" for "i" (diagonal above) 
\end{itemize}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Illustration}
\textbf{Step 3} for each row , for each column ...
\begin{table}
\begin{center}
\begin{tabular}{llllllllllll}
\toprule
 &  & e & x & e & c & u & t & i & o & n \\
\midrule
 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
i & 1 & 1 & \textbf{2} & 3 & 4 & 5 & 6 & 6 & 7 & 8 \\
n & 2  \\
t & 3 \\
e & 4 \\
n & 5 \\
t & 6 \\
i & 7 \\
o & 8 \\
n & 9 &  &  &  &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Converting "ex" to "i": min of 

\begin{itemize}
\item Converting "e" to "i", plus deletion (left cell)
\item converting "ex" to "empty string", plus insertion (upper cell)
\item Converting "e" to "empty string", plus substitution of "x" for "i" (diagonal above) 
\end{itemize}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Illustration}

\textbf{Step 3} for each row , for each column ...

\begin{table}
\begin{center}
\begin{tabular}{lllllllllll}

\toprule
 &  & e & x & e & c & u & t & i & o & n \\
\midrule
 & \textbf{0} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
i & 1 & \textbf{1} & 2 & 3 & 4 & 5 & 6 & 6 & 7 & 8 \\
n & 2 & 2 & \textbf{2} & 3 & 4 & 5 & 6 & 7 & 7 & 7 \\
t & 3 & 3 & 3 & \textbf{3} & 4 & 5 & 5 & 6 & 7 & 8 \\
e & 4 & 3 & 4 & 3 & \textbf{4} & 5 & 6 & 6 & 7 & 8 \\
n & 5 & 4 & 4 & 4 & 4 & \textbf{5} & 6 & 7 & 7 & 7 \\
t & 6 & 5 & 5 & 5 & 5 & 5 & \textbf{5} & 6 & 7 & 8 \\
i & 7 & 6 & 6 & 6 & 6 & 6 & 6 & \textbf{5} & 6 & 7 \\
o & 8 & 7 & 7 & 7 & 7 & 7 & 7 & 6 & \textbf{5} & 6 \\
n & 9 & 8 & 8 & 8 & 8 & 8 & 8 & 7 & 6 & \textbf{5} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\url{http://www.let.rug.nl/kleiweg/lev/}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{With Substition Cost of 2}
\begin{table}
\begin{center}
\begin{tabular}{lllllllllll}

\toprule
 &  & e & x & e & c & u & t & i & o & n \\
\midrule
 & \textbf{0} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
i & \textbf{1} & 2 & 3 & 4 & 5 & 6 & 7 & 6 & 7 & 8 \\
n & 2 & \textbf{3} & 4 & 5 & 6 & 7 & 8 & 7 & 8 & 7 \\
t & 3 & 4 & \textbf{5} & 6 & 7 & 8 & 7 & 8 & 9 & 8 \\
e & 4 & 3 & 4 & \textbf{5} & \textbf{6} & 7 & 8 & 9 & 10 & 9 \\
n & 5 & 4 & 5 & 6 & 7 & \textbf{8} & 9 & 10 & 11 & 10 \\
t & 6 & 5 & 6 & 7 & 8 & 9 & \textbf{8} & 9 & 10 & 11 \\
i & 7 & 6 & 7 & 8 & 9 & 10 & 9 & \textbf{8} & 9 & 10 \\
o & 8 & 7 & 8 & 9 & 10 & 11 & 10 & 9 & \textbf{8} & 9 \\
n & 9 & 8 & 9 & 10 & 11 & 12 & 11 & 10 & 9 & \textbf{8} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}



\url{http://www.let.rug.nl/kleiweg/lev/}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Finding near matches for messy data...}
\begin{itemize}

\item Edit distance is a powerful tool to remove typos due to erroneous or bad quality scanned text data.

\item A lot of social program data records are (still) paper based and need to be scanned in.

\item Scanning errors are usually not linguistic in nature, but rather consist of character omissions.


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Measuring Political Turnover: Raw CIA data}
\begin{figure}[h]
\begin{center}$
\begin{array}{cc}
\includegraphics[scale=0.17]{figures/ocr-example-cia1.png} &
\includegraphics[scale=0.15]{figures/ocr-example-cia2.png}
\end{array}$
\end{center}
\caption{CIA Reports Tracking Political Transitions}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Measuring Political Turnover: Raw CIA data}
\begin{figure}[h]
\begin{center}$
\begin{array}{cc}
\includegraphics[scale=0.3]{figures/ocr-scan1.png} &
\includegraphics[scale=0.3]{figures/ocr-scan2.png}
\end{array}$
\end{center}
\caption{CIA Reports Tracking Political Transitions}
\end{figure}

$\Rightarrow$ 50 years of monthly data, essentially covering all countries of the world.
$\Rightarrow$ 3 million rows of raw data, initially 342,540 unique rows.
$\Rightarrow$ Levenshtein based dimensionality reduction reduces this down to 199,028.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Measuring Political Turnover: Raw CIA data}
 It is evident that many strings are very very similar, and since typos are idiosyncratic to an individual document, we can take a frequentist approach.

<<levenshteinillustration,tidy=TRUE, size="tiny", eval=TRUE, include=TRUE,echo=TRUE,fig.show="hide">>=
library(RecordLinkage)
levenshteinDist( "MIN OF EDUCATION	PUPAL AL I AHMED	I","MIN OF EDUCATION	POPAL, ALI AHMAD H")

levenshteinSim("MIN OF EDUCATION	PUPAL AL I AHMED	I","MIN OF EDUCATION	POPAL, ALI AHMAD H")

##run on whole vector
VEC<-c("CHIEF OF STATE	ZAHIR SHAH, MUHAMMAD KING	H","PRIME MINISTER	DAUD, MUHAMMAD SARDAR	H","FIRST DEPUTY PRIME MINISTER	MUHAMMAD» ALI SARDAR	H","SECOND DEPUTY PRIME MINISTER	NA IM».MUHAMMAD SARDAR	H","MIN OF AGRICULTURE	ADALAT » GHOLAM HAIDER	H","MIN OF COMMERCE	'	. SHERZAD♦ GHOLAM MUHAMMAD	H","^	MIN OF COMMUNICATIONS	MORID, MUHAMMAD	H","MIN OF COURT	.	ALI, SULEIMAN AHMAD SARDAR	■","MIN OF DEFENSE	DAUD, MUHAMMAD SARDAR	H","MIN OF EDUCATION	POPAL, ALI AHMAD	H","^ , MIN OF FINANCE	MALIKYAR, ABDULLAH	H","MIN OF FOREIGN AFFAIRS	NAIM, MUHAMMAD SARDAR	■","MIN OF INTERIOR	ABDULLAH, SAYYED	■","MIN OF JUSTICE	ABDULLAH, SAYYED	H","MIN OF MINES & INDUSTRIES	YUSEF, MIR MUHAMMAD DR	:	■","^	MIN OF PLANNING	DAUD, MUHAMMAD SARDAR	I","MIN	OF	PUBLIC	HEALTH	■","MIN	OF	PUBLIC	WORKS	H","DEPT OF PRESS	SOHEIL, MUHAMMAD ASEF	H","DEPT OF TRIBAL AFFAIRS	MAJRUH, SAYYED SHAMSUDDIN	H")
SIM<-levenshteinSim("MIN OF EDUCATION	PUPAL AL I AHMED	I",VEC)

SIM

VEC[which.max(SIM)]
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Clustering Based on Edit Distance}
Clustering is a very useful machine learning application that typically requires distance objects. Working with text data often requires a disambiguation of alternative spelling varations and clustering can be a very useful tool.

\begin{center}
\includegraphics[scale=0.4]{figures/openrefine.png}
\end{center}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{OpenRefine Clustering}


\begin{center}
\includegraphics[scale=0.3]{figures/clustering-data-cleaning.png}
\end{center}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

