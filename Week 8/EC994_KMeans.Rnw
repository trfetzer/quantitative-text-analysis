\documentclass{beamer}
\usetheme{default}
%\usetheme{Malmoe}

\title[EC999: Quantitative Text Analysis]{EC999: K-Means Clustering} \def\newblock{\hskip .11em plus .33em minus .07em}


\def\Tiny{\fontsize{10pt}{10pt}\selectfont}
\def\smaller{\fontsize{8pt}{8pt}\selectfont}

\institute[Warwick]{University of Chicago \& University of Warwick}
\author[Thiemo Fetzer]{Thiemo Fetzer}

 \date{\today}

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{graphics}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{natbib}
\usepackage{hyperref}
%\usepackage{enumitem}
 \usepackage{pgffor}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}
\usepackage{verbatim} 
\usepackage{cancel}
\newcommand\xxcancel[1]{\xcancel{#1}\vphantom{#1}}

\usepackage{mathtools,xparse}
 
 \addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\setbeamersize{text margin left = 16pt, text margin right = 16pt}

\newenvironment<>{algorithm}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Algorithm}%
\else%
   \def\insertblocktitle{Algorithm ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=yellow!50!black}
  \setbeamercolor{block body}{fg=black,bg=yellow!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}


\newenvironment<>{assumption}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Assumption}%
\else%
   \def\insertblocktitle{Assumption ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=blue!50!black}
  \setbeamercolor{block body}{fg=black,bg=blue!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}

\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(data.table)
library(calibrate)
library(plyr)
opts_chunk$set(fig.path='figures/knitr-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
options(scipen = 1, digits = 3)
setwd("~/Dropbox/Teaching/Quantitative Text Analysis/FINAL/Week 8")
library(glmnet)
library(ggplot2)
@

\AtBeginSection[]
{
 \begin{frame}<beamer>
 \frametitle{Plan}
 \tableofcontents[currentsection]
 \end{frame}
}
\maketitle
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Clustering}
 
\begin{itemize}

\item So far we have talked about numerical prediction/ classification and some methods to help you arrive at robust models. The focus was on understasnding $Y|X$

\item The last section of the course talks about dimensionality reduction in the broadest sense.

\item Dimensionality reduction can be thought of as reducing patterns in $\mathbf{X}$ and mapping them to a lower dimensonality subspace; such patterns could be group membership.

\item Clustering is a form of dimensionality reduction: we want to group data together that is ``similar'' by some metric.

\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Clustering Example: Google News}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.35]<1>{figures/clustering-1.png} 
\includegraphics[scale=.35]<2>{figures/clustering-2.png} 
\end{array}$
\caption{Google News as Example of Cluster}
\end{center}
\end{figure}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Clustering Example: Data Cleaning}


\begin{figure}[h]
\begin{center}$
\begin{array}{cc}
\includegraphics[scale=.35]<1>{figures/kw2.png} & \includegraphics[scale=.35]<1>{figures/kw3.png} 
\end{array}$
\caption{Raw Data}
\end{center}
\end{figure}
\tiny{
MIN OF FOREIGN AFFAIRS AL SABAH JABIR AL AHMAD AL JABIR \\
Min of Foreign Affairs Sabah Jabir al Ahmad al Jabir al \\
Min of Foreign   Sabah Sabah al Ahmad al Jabir al \\ 
Min of Foreign Affairs Sabah Sabah Ahmad Jabir al \\
Min of Foreign Affairs Sabah Sabah Ahmad Jabir Al \\ 
Min of Foreign Affairs Sabah sabah al Ahmad al Jabir Al \\
Min of Foreign Affairs Sabah Sabah al Ahmad al Jabir \\
Min of Foreign Affairs Sabah SABAH al Ahmad al Jabir Al \\
Min of Foreign Affairs Sabah SABAH al Ahmad al Jabir al 
}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Clustering Example: Data Cleaning}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.22]<1>{figures/clustering-data-cleaning.png} 
\end{array}$
\caption{Open Refine for Working with Messy Data: Clustering Text}
\end{center}
\end{figure}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{What are good clusters?}
 
\begin{itemize}

\item A good clustering method will provide...
\begin{itemize}
\item High Intra cluster similarity: cohesive within clusters

\item Low Inter class similarity: distinctive between clusters.
\end{itemize}

\includegraphics[scale=.25]<1>{figures/inter-intra.png} 


\item The quality of a clustering method depends on
\begin{itemize}
\item the similarity measure used by the method

\item its implementation, and 

\item Its innate ability to discover hidden patterns.
\end{itemize}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{K-Means Clustering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means: One dimensional case}

\begin{itemize}

\item Suppose $\mathbf{X}$ is one dimensional.
\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.5]<1>{figures/onedimensional1.png} 
\includegraphics[scale=.5]<2>{figures/onedimensional2.png} 
\includegraphics[scale=.5]<3>{figures/onedimensional3.png} 
\end{array}$
\caption{Kernel Density of Hypothetical Mixture Distribution of five Normals, taken from Matt Taddy.}
\end{center}
\end{figure}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering}

\begin{itemize}

\item Let $C_1,...C_K$ denote sets containing the indices of the observations in each cluster, i.e. each $\mathbf{x_j}$ is assigned to a cluster $C_i$.

\item the partitioning of the data is non-overlapping and exhaustive:

$$C_1 \cup C_2 \cup ... \cup C_K = \{1,...,n\}$$
$$\text{for } k \neq k': C_k \cap C_{k'} = \emptyset$$

\item $K$-Means clustering attempts to minimize the \emph{within-cluster} variation, that is, choose an assignment rule $C_1,...C_K$ such that:

$$\min_{C_1,...,C_k}{\sum_{k=1}^{K}{W(C_k)}}$$

\item where we can measure distance between points within a cluster using squared Euclidian distance ($\mathcal{L}_2$ norm).
$$W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k}\sum_{j=1}^p{(x_{ij}-x_{i'j})^2}$$

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Within Cluster Distance}

$$W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k}\sum_{j=1}^p{(x_{ij}-x_{i'j})^2}$$

\begin{itemize}

\item  Compute all pairwise distances between all points $i, i' \in C_k$ for all $k$.  

\item  There is a trick, you can show that:

\end{itemize}

$$W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k}\sum_{j=1}^p{(x_{ij}-x_{i'j})^2} = 2 \sum_{i \in C_k}\sum_{j=1}^p{(x_{ij}-\bar{x}_{kj})^2}$$

[Proof (idea) is simple/ draw]:
Suppose two points on uniform line belonging to a single cluster, i.e. $x_1,x_2$, then the above relationship is:

$$\underbrace{\frac{1}{2}[(x_1-x_2)^2 + (x_2 -x_1)^2]}_{\text{all pairwise combinations}} = 2 [ (x_1 - \bar{x})^2  + (x_2 - \bar{x})^2]$$


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering}

\begin{itemize}

\item  Overall objective function can be rewritten as 
$$\min_{C_1,...,C_k}{\sum_{k=1}^{K}{\sum_{i \in C_k}\sum_{j=1}^p{(x_{ij}-\bar{x}_{kj})^2}}}$$

\item Using $\mathcal{L}_2$ norm notation
$$\min_{C_1,...,C_k}{\sum_{k=1}^{K}{\sum_{i \in C_k} \lVert \mathbf{x}_{i}-\bar{x}_{k} \rVert_{2}^2 }}$$


\item So what are we trying to optimize? By choosing assignments of points $\mathbf{x_1},...\mathbf{x_n}$ to $K$ clusters, we want to minimize the across cluster sum of squared deviations. 

\item This is a very difficult problem to solve... for a given number of $K$ topics, there are $K^n$ ways of partitioning the data into $K$ clusters.

\item However, there are algorithms that help find \emph{locally optimal} solutions.
 
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering Algorithm}

 \begin{algorithm}[K-Means Clustering Algorithm]
   \begin{enumerate}
   
   \item Initialize by randomly assigning each observation to a cluster $K$, i.e. create at random $C_1,...,C_K$.
   
   \item Continue the following steps, and only stop when the assignments of observations to clusters $C_1,...,C_K$ does not change anymore: 
   
   \item[] a) For each cluster $k \in K$, compute the centroid $\mathbf{\bar{x}_k}$.
   
   \item[] b) Assign each observation to the cluster whose centroid is closest (as defined by Euclidian distance). 
   
   
   \end{enumerate}
\end{algorithm}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means: Two dimensional algorithm illustration}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.55]<1>{figures/kmeans-illustration.png} 
\end{array}$
\caption{Two dimensional example illustrating sequence of iterations.}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{What guarantees convergence?}

\begin{itemize}

\item At each iteration step, the objective function will improve or stay the same, why?

$$\min_{C_1,...,C_k}{\sum_{k=1}^{K}{\sum_{i \in C_k}\sum_{j=1}^p{(x_{ij}-\bar{x}_{kj})^2}}}$$

\item In Step 2(a) the cluster means for each feature are the constants that minimize the sum-of-squared deviations, and in Step 2(b), reallocating the observations can only improve.

\item This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective function will never increase.

\item However, the convergence is to a \emph{local minimum}, not a global minimum: what should we do? 
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{K-Means: Choosing $K$}
<<kmeansexamplecode, echo=FALSE, message=FALSE, warning=FALSE,results="hide", size="tiny">>=
set.seed(2014)

library(dplyr)
TRUECENTERS <-data.frame( x1=c(1.5, 0, -1.75), x2=c(-1, .5, -0.75))
centers <- data.frame(cluster=as.numeric(1:3), size=c(200, 150, 80), x1=TRUECENTERS[,1], x2=TRUECENTERS[,2])
points <- centers %>% group_by(cluster) %>%
    do(data.frame(x1=rnorm(.$size[1], .$x1[1]),
                  x2=rnorm(.$size[1], .$x2[1])))

plot(x2 ~ x1, col=as.factor(cluster), data=points, cex=.5)
points(x2 ~ x1, data=TRUECENTERS, pch=2)


@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{K-Means: Choosing $K$}
<<kmeansmultipleks, echo=FALSE, message=FALSE, warning=FALSE,results="hide", size="tiny", fig.show='hide'>>=


set.seed(1990)

TSS<-NULL
for(k in 1:10) {
kclust <- kmeans(points[,c(2,3)], k)
plot(x2 ~ x1, col=as.factor(kclust$cluster), data=points, cex=.5, main=paste("K = ",k))
TSS<-rbind(TSS,kclust$tot.withinss)
}
@

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.5]<1>{figures/knitr-kmeansmultipleks-2.pdf} 
\includegraphics[scale=.5]<2>{figures/knitr-kmeansmultipleks-4.pdf}
\includegraphics[scale=.5]<3>{figures/knitr-kmeansmultipleks-8.pdf}
\end{array}$
\caption{Example K-Means on simulated data with different values for $K$.}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{How to optimally choose the number of clusters $K$?}

\begin{itemize}

\item<1-> Suppose you set $K=N$: What is our objective function value?

$$\min_{C_1,...,C_k}{\sum_{k=1}^{K}{\sum_{i \in C_k}\sum_{j=1}^p{(x_{ij}-\bar{x}_{kj})^2}}}$$
\item<2-> It would be zero!

\item<3-> Is there a way that we can validate the choice of $K$? No validation set, because we dont know the true number of clusters or have a good benchmark.

\item<3-> Visual selection using the previous scree plot

\item<3-> Gap statistic is theoretically motivated in Hastie et al. (2001). ``Estimating the number of clusters in a data set via the gap statistic.'' 

\item<4-> Some rules of thumb suggest $K = \sqrt{n/2}$, where $n$ is the number of observations.

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{Evolution of Objective function as function of $K$}
<<kmeansmultipleksplottss, echo=FALSE, message=FALSE, warning=FALSE,results="hide", fig.show='hide', size="tiny">>=
plot(TSS)
lines(TSS)
plot(TSS)
lines(TSS)
points(x=3,y=TSS[3,], pch=19, col="red")
temp<-TSS-lag(TSS)
plot(temp)
lines(temp)
plot(temp-lag(temp))
lines(temp-lag(temp))
@
\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.5]<1>{figures/knitr-kmeansmultipleksplottss-1.pdf} 
\includegraphics[scale=.5]<2>{figures/scree.pdf}
\includegraphics[scale=.5]<3>{figures/knitr-kmeansmultipleksplottss-2.pdf}
\end{array}$
\caption{Scree Plot and Elbow points.}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{Formal(ish) intuition for Elbow points}
 \begin{itemize}
 
 \item Scree plot: Mountain peak stops, rubble starts.
 \item At elbow point, here $K=3$, the rate at which objective function changes, changes a lot.
 \item Think of the second derivative of objective function" elbow point is the point where the second derivative reaches a local maximum.
 
 \end{itemize}
 
 
\begin{figure}[h]
\begin{center}$
\begin{array}{ccc}
\includegraphics[scale=.2]{figures/knitr-kmeansmultipleksplottss-2.pdf} 
\includegraphics[scale=.2]{figures/knitr-kmeansmultipleksplottss-3.pdf}
\includegraphics[scale=.2]{figures/knitr-kmeansmultipleksplottss-4.pdf}

\end{array}$
\caption{Scree Plot and Elbow points.}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering: An application from research}

\begin{itemize}

\item In Fetzer and Marden (2016), we study the impact of conservation units in Brazil on land related conflict. 

\item The idea in a nutshell: conservation units assign property rights; once property rights are assigned, people need not fight over property titles to land. 

\item Categorize land into three classes $\{ F, S, C\}$, $F$orested, $S$hrubland, $C$ropland.
 
\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.35]<1>{figures/brazil-protected-areas.png} 
\includegraphics[scale=.25]<2>{figures/landcover-C-plain.png}  \includegraphics[scale=.25]<2>{figures/landcover-C.png}
\includegraphics[scale=.25]<3>{figures/landcover-S-plain.png}  \includegraphics[scale=.06]<3>{figures/landcover-S.png}
\end{array}$
\caption{}
\end{center}
\end{figure}

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering: An application from research}

\begin{itemize}

\item We have around 12 years of data and the time series tells us something about how each land pixel is being used; we want to look how conservation units affect \emph{within pixel} land use.

\item Always forested FFFFFFF, Illegal logging FFFSFFF, Land clearing then rotating crop use FFFSCSCSC, etc.

\item Conservation units, without enforcement, should only affect stationary banditry.

\item We want to cluster the data into three groups: roving bandits, stationary bandits and unused. So dimensionality reduction from $4^5 =1,024$ possible combinations to three clusters.

\item We create some numeric features representing five letter length series: dummy variable if pixel ever $C$, length of repeating state, length of repeating state pair (e.g. SCSC has SC repeating twice), number of times pixel is forested.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering: An application from research}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/brazil-tss.pdf}
\includegraphics[scale=.5]<2>{figures/kmeans-sep2.png} 
\includegraphics[scale=.45]<3>{figures/kmeans-sep1.png} \includegraphics[scale=.45]<4>{figures/kmeans-sep3.png}
\end{array}$
\caption{Separation Achieved to Cluster Sequences of Land Use Patterns}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Means Clustering Issues}

\begin{itemize}

\item<1-> K-Means works with quantitative data only, but some data may be categorical.

\item<2-> Similarity is hard to define and typically, assessment requires human judgement of the sensibility of detected clusters. Euclidian distance may not be a good way to measure distance, and may not be defined for non -numeric features!


\item<3-> Its not suitable to detect clusters in non convex shapes.
\includegraphics[scale=.45]<3>{figures/pure_kmeans-nonconvex.png} 

\item<4-> K-Means as presented here is a \emph{hard clustering} approach; it forces every observation into a cluster; this is problematic if there are outliers, as means change a lot.
\includegraphics[scale=.45]<4>{figures/distorted-outliers.png} 

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Some hands-on R code on clustering}
 
<<clustering, size="tiny",eval=TRUE, tidy=TRUE>>=
library(cluster)
library(quanteda)
load(file="../../Data/trumpstweets.rdata")
# remove retweet entities
tw.user.df$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tw.user.df$text)
# remove at people
tw.user.df$text = gsub("@\\w+", "", tw.user.df$text)
# remove punctuation
tw.user.df$text = gsub("[[:punct:]]", "", tw.user.df$text)
# remove numbers
tw.user.df$text = gsub("[[:digit:]]", "", tw.user.df$text)
# remove html links
tw.user.df$text = gsub("http\\w+", "", tw.user.df$text)
# remove unnecessary spaces
tw.user.df$text = gsub("[ \t]{2,}", "", tw.user.df$text)
tw.user.df$text = gsub("^\\s+|\\s+$", "", tw.user.df$text)

trump.dfm<-dfm(tw.user.df$text, remove=stopwords(), stem=TRUE)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{K-Means on Trump tweets}
 
<<clusteringdistance, size="tiny",eval=TRUE, tidy=TRUE>>=
## k-means clustering
set.seed(18022017)
trump.dfm.trim <- dfm_trim(trump.dfm, min_count = 3, min_docfreq = 2)

# some guesses for "optimal k is sqrt of number of documents
k <- round(sqrt(ndoc(trump.dfm.trim)/2))

#a bit too many
k
#computing words as shares of words in total document is like normalising length of documents, so can use Euclidian distance.

#tf function converts word count dfm to share
clusterk5 <- kmeans(tf(trump.dfm.trim, "prop"), 5)
splits<-split(docnames(trump.dfm.trim), clusterk5$cluster)
unlist(lapply(splits, length))
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{K-Means on Trump tweets}
<<clusteringdistance2, size="tiny",eval=TRUE, warning=FALSE, tidy=TRUE>>=
## k-means clustering
textplot_wordcloud(dfm_trim(trump.dfm.trim[splits[[3]]], min_count = 1))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
    

\section{K-medoids Clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Improving on K-Means Clustering}

\begin{itemize}

\item Sensitivity of K-Means to outliers can distort the results dramatically: why? the inclusion of an outlier in any cluster, for small number of $N$ can change the mean a lot.

\item What can you do? Rather than computing a centroid, we can just pick a ``representative observation''.

\item In the K-Means clustering step, means are computed in step 2a).

\item Alternatively to computing the means, you can simply select a \emph{most representative} point out of the points that are allocated to a specific cluster $\rightarrow$ \emph{medoids}.

\item Select a ``representative point'', rather than compute a mean.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{K-Medoids Clustering Algorithm}

 \begin{algorithm}[K-Medoids Clustering Algorithm]
   \begin{enumerate}
   
   \item Initialize by randomly selecting $K$ points from $N$ as the \emph{medoids} $\mathbf{m} = \{\mathbf{m}_1,...,\mathbf{m}_{K}\}$ and assign each observation to the closest medoid, where closest is defined by some dissimilarity metric $D(\mathbf{x}_{i}, \mathbf{x}_{i'})$. 
   
   \item \textbf{Medoid selection step}: For a given cluster assignment $C_1,...,C_K$, find for each cluster $k$ the observation $\mathbf{m}_k$, that minimizes the total distance to the other points belonging to this cluster, i.e. find for each $k$:
   
   $$ i^*_k = argmin_{\{ i \in C_k\}}{\sum_{i' \in C_k}{D(\mathbf{x}_i, \mathbf{x}_{i'})} }$$
   
   \item \textbf{Cluster assignment step}: Given a set of medoids $\{\mathbf{m_1},..,\mathbf{m_K}\}$

\item Continue step 2,3 until the assigments do not change anymore.

   \end{enumerate}
\end{algorithm}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{How do we measure distance?}
 
\begin{itemize}

\item This is not straightforward. Typically, we want a distance function $D(\mathbf{x},\mathbf{y})$ to satisfy three properties.

\begin{enumerate}

\item Nonnegativity: $D(\mathbf{x},\mathbf{y}) \geq 0$
\item Symmetry: $D(\mathbf{x},\mathbf{y}) = D(\mathbf{y},\mathbf{x})$
\item Triangle Inequality: $D(\mathbf{x},\mathbf{z})  \leq D(\mathbf{x},\mathbf{y}) +  D(\mathbf{y},\mathbf{z})$

\end{enumerate}

\item There are many functions that satisfy this property. In the clustering section, instead of working in feature space $X$, we look at distance matrices.

\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Dissimilarity Matrix}
Suppose you have some matrix $\mathbf{X}$ with dimension $n \times p$ and you compute all pairwise distances between any points, using \emph{any distance metric you like}.

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.3]<1>{figures/distance-matrix.png} 
\includegraphics[scale=.3]<2>{figures/distance-matrix-dissimilar-1.png} 
\includegraphics[scale=.3]<3>{figures/distance-matrix-dissimilar-2.png}
\end{array}$
%\caption{}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Can we perform k-medoids clustering on distance matrices?}
 
\begin{itemize}

\item The answer is: Yes!

\item All we do is assign points as medoids and then, reassign points to their closest medoids.

\item We do not need to know the information about the actual features $X$ that underly the distance matrix, but rather, we \emph{only} need to know their pairwise distances, i.e. the data dissimilarity matrix $D$ which has dimensions $n \times n$.

\item This greatly improves the set of potential applications for k-Means clusteirng, as you can define \emph{any distance}.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 


  

 

\end{document}
