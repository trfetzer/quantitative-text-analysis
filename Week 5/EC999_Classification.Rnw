\documentclass{beamer}
\usetheme{default}
%\usetheme{Malmoe}

\title[EC999: Quantitative Text Analysis]{EC999: Classification} \def\newblock{\hskip .11em plus .33em minus .07em}


\def\Tiny{\fontsize{10pt}{10pt}\selectfont}
\def\smaller{\fontsize{8pt}{8pt}\selectfont}

\institute[Warwick]{University of Chicago \& University of Warwick}
\author[Thiemo Fetzer]{Thiemo Fetzer}

 \date{\today}

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{graphics}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{natbib}
\usepackage{hyperref}
%\usepackage{enumitem}
 \usepackage{pgffor}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}
\usepackage{verbatim} 
\usepackage{cancel}
\newcommand\xxcancel[1]{\xcancel{#1}\vphantom{#1}}

\usepackage{mathtools,xparse}

\newenvironment{Description}
               {\list{}{\labelwidth=0pt \itemindent-\leftmargin
                        \let\makelabel\Descriptionlabel
                        % or whatever
               }}
               {\endlist}
\newcommand*\Descriptionlabel[1]{%
  \hspace\labelsep
  \normalfont%  reset current font setting
  \color{blue}\bfseries\sffamily% or whatever 
  #1}


\setbeamersize{text margin left = 16pt, text margin right = 16pt}
\newcommand{\code}[1]{\texttt{#1}}

\newenvironment<>{algorithm}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Algorithm}%
\else%
   \def\insertblocktitle{Algorithm ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=yellow!50!black}
  \setbeamercolor{block body}{fg=black,bg=yellow!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}


\newenvironment<>{assumption}[1][\undefined]{%
\begin{actionenv}#2%
\ifx#1\undefined%
   \def\insertblocktitle{Assumption}%
\else%
   \def\insertblocktitle{Assumption ({\em#1})}%
\fi%
\par%
\mode<presentation>{%
  \setbeamercolor{block title}{fg=white,bg=blue!50!black}
  \setbeamercolor{block body}{fg=black,bg=blue!20}
}%
\usebeamertemplate{block begin}\em}
{\par\usebeamertemplate{block end}\end{actionenv}}



%changing spacing between knitr code and output
\usepackage{etoolbox} 
\makeatletter 
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt } 
\makeatother
\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}



\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(data.table)
library(calibrate)
library(plyr)
library(reshape2)
opts_chunk$set(fig.path='figures/knitr-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
options(scipen = 1, digits = 3)
options(warn=-1)
setwd("~/Dropbox/Teaching/QTA/Lectures/Week 5")
options(stringsAsFactors = FALSE)
library(glmnet)
library(ggplot2)
library(data.table) 
library(RJSONIO)
library(quanteda)
stopwords=c("this","a","the","and","for")

library(RTextTools)
library(data.table)
library(plyr)
library(lubridate)
library(e1071)
textmat <-
function (vec=A, stopws=stopwords) {
        dummy <- mapply(wordfreq, vec, 1:length(vec), MoreArgs=list(stopws=stopwords), SIMPLIFY=F)
        names(dummy) <- NULL
        dtm <- t(xtabs(Freq ~ ., data = do.call("rbind", dummy)))
        dtm
}

str_break = function(x, width = 90L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}

library(tm)

library(knitr)
library(data.table)
library(ggplot2)
library(gridExtra)
library(reshape)
library(plyr)
library(boot)
library(pROC)

opts_chunk$set(fig.path='figures/knitr-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
options(scipen = 1, digits = 3)
options(oma = c(5,4,0,0) + 0.1,  mar = c(0,0,1,1) + 0.1)
options(stringsAsFactors = FALSE)
load("R/MORTGAGE.rdata") 

MORTGAGE<-MORTGAGE[ApplicantIncome<200 & LoanAmount<500 ]
MORTGAGE<-MORTGAGE[order(Denied)]
MORTGAGE<-MORTGAGE[Leverage<10]
MORTGAGE<-MORTGAGE[!is.na(ApplicantIncome) & !is.na(LoanAmount)]
MORTGAGE<-MORTGAGE[ApplicantSex ==1 | ApplicantSex==2]
MORTGAGE[, Female:= as.numeric(ApplicantSex==2)]
setnames(MORTGAGE, "NonWhite","Minority")
MORTGAGE$Occupancy<-as.character(MORTGAGE$Occupancy)
MORTGAGE<-MORTGAGE[Occupancy %in% c("1","2")]
MORTGAGE[Occupancy==1, Occupancy := "Owner-occupied"]
MORTGAGE[Occupancy==2, Occupancy := "Non Owner-occupied"]
MORTGAGE$Occupancy<-as.factor(MORTGAGE$Occupancy)
MORTGAGE$LoanPurpose<-as.character(MORTGAGE$LoanPurpose)
MORTGAGE[LoanPurpose==1, LoanPurpose := "Home purchase"]
MORTGAGE[LoanPurpose==2, LoanPurpose := "Home improvement"]
MORTGAGE[LoanPurpose==3, LoanPurpose := "Refinancing"]
MORTGAGE$LoanPurpose<-as.factor(MORTGAGE$LoanPurpose)
MORTGAGE$STATE_FIPS<-factor(MORTGAGE$STATE_FIPS)

MORTGAGE$Deniedfactor<-"Granted"
MORTGAGE[Denied==TRUE]$Deniedfactor<-"Denied"

MORTGAGE$CoApplicant<-MORTGAGE[, CoApplicantRace!=8]

@

\AtBeginSection[]
{
 \begin{frame}<beamer>
 \frametitle{Plan}
 \tableofcontents[currentsection]
 \end{frame}
}
\maketitle
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regression versus Classification}

\begin{itemize}

\item Classification refers to cases where the $y_i$ is a categorical variable, such as Eye color, Gender, Brand, Sentiment = Positive, Negative, Neutral. 

\item Regression refers to cases where the dependent variable is numeric, like price, quantity, ...

\item There are cases, where categorical variables can be given a numeric interpretation, e.g. a categorical variable with two levels can be expressed as a dummy variable/ binary variable, such as Gender = 1 if male, Gender = 0 if female.

\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Classification Example (1): Land Use Patterns}
  \begin{figure}[h]
\begin{center}
\includegraphics<1>[scale=.5]{figures/landcover-C-plain.png} 
 \includegraphics<2>[scale=.5]{figures/landcover-C.png} 
\end{center}
\caption{\small{Classifiying pixels from satellite imagery into common land use clusters: Cropland, Forest and Shrubland as used in Fetzer and Marden (2016).}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Other Classification Examples}

\begin{enumerate}

\item Fraud detection in financial transaction data

\item News Feed categorization 

\item Email Spam detection

\item Facebook Image Upload Nudity detection

\item Sentiment categorization 

\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The General Classification Problem}

\begin{itemize}

\item A qualitative variable takes a value in set $\mathcal{C}$.

\item We will denote $|\mathcal{C}| = c$ the number of different categories.

\item We observe a feature vector $X$ and a responsive variable $Y$ that takes on values from the set $\mathcal{C}$

\item We estimate for each possible value $y \in \mathcal{C}$:

$$ \hat{P}(Y=y|X) $$

\item Following a decision rule, we will assign the ``correct'' label to that category or class $y$, which reduces some form of prediction error.

\item The prediction error we can make here is the share of ``wrongly`` classified categories...

\item How to measure ``wrongly classified'' or ``correct'' label?

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Validation Set Approach to Assess Model Accuracy}
  \begin{figure}[h]
\begin{center}
\includegraphics<1>[scale=.35]{figures/validation-set.png} 
\end{center}
\caption{\small{Validation Set Workflow Illustration.}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

\section{Bayes Rule Reminder}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bayes Theorem}

\begin{itemize}

\item Bayes Rule States
 $$\underbrace{P(Y|X)}_{posterior} = \frac{\overbrace{P(X|Y)}^{likelihood} \times \overbrace{P(Y)}^{prior}}{\underbrace{P(X)}_{marginal\;likelihood}}$$

\item In case $Y$ and $X$ where independent, $P(Y|X) = P(Y)$, and $P(Y \cap X) = P(Y) P(X)$.

\item $X$ is the evidence, that tells you something about the probability of observing the data $Y$.

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bayes Theorem: Classical Problem}


\begin{quote}
A patient takes a lab test and the result comes back positive. The test returns a correct positive result in only 98\% of the cases in which the disease is actually present, and a correct negative result in only 97\% of the cases in which the disease is not present. You know that 0.008 of the entire population have this disease.
\end{quote}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Bayes Theorem: Classical Problem}

What are we given? Let $D$ be an indicator if the disaese is present and $+$ indicate whether the test is positive.

\begin{eqnarray*}
P(D) &=& 0.008 \\
P(\neg D) &=& 1-0.008 \\
P(+ | D) &=& 0.98\\
P(- | D) &=& 0.02\\
P(+ | \neg D) &=& 0.03 \\
P(- | \neg D) &=& 0.97
\end{eqnarray*}

What is $P(D | +)$?

$$P(D | +) = \frac{P(D) P(+|C)}{P(+)}$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Law of Total Probability}

$$P(D | +) = \frac{P(D) P(+|D)}{P(+)}$$

Using the law of total probability

$$P(+) = P(+|D) P(D) + P(+| \neg D) P(\neg D)$$

$$= 0.98 \times 0.008 + 0.03 \times 0.992 = 0.0376 $$

From this you get:

$$ P(D|+) =  \frac{P(D) P(+|D)}{P(+)}{P(+)} = \frac{ 0.008 \times 0.98}{0.0376} = 0.21$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bayesian Decision Theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Prior Information}

\begin{itemize}

\item Suppose you want to categorize some \emph{teaching feedback} into two categories.

\item In our example we have $Y \in \{\text{Positive}, \text{Negative}\}$.

\item Suppose you know what the \emph{priors} are, i.e. you know what the share of positive and negative feedbacks are \emph{in the population}.

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/all-feedbacks.png}
\end{array}$
\end{center}
\end{figure}

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Best Decision Without Additional Information}

\begin{itemize}

\item Suppose you now receive a new feedback $\tilde{Y}$, and you want to categorize it \emph{without any additional information}

\item What decision / assignment rule would minimize the missclassification error?

\end{itemize}

Clearly this should be

$$ \tilde{Y} = \begin{cases} +  &\mbox{if } P(+) > P(-)  \\ 
 - & \mbox{if } P(+) < P(-)  \end{cases}   $$

What is the error that we make with this \emph{decision rule}?

$$P(error) = \min\{P(+), P(-) \}$$

So here, we would assign any feedback coming in as negative $-$; the maximum error we can make is 18\%.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Best Decision With Additional Information}

\begin{itemize}

\item Suppose you now have additional information $X$, let that information be the number of characters contained in a feedback. 

\item Suppose that the distribution of the number of characters is smooth in the population. 

\item Plot the density function $p(x|+)$ and $p(x|-)$.

\end{itemize}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/class-conditionals.png}
\end{array}$
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Using Bayes Rule}

\begin{itemize}

\item For every value of $x$, we can compute the \emph{posterior} probability using Bayes rule.

 $$\underbrace{P(+|x)}_{posterior} = \frac{\overbrace{p(x|+)}^{likelihood} \times \overbrace{P(+)}^{prior}}{\underbrace{p(x)}_{marginal\;likelihood}}$$

where $$p(x) = p(x|+) P(+) + p(x|-) P(-)$$

\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Using Bayes Rule}

\begin{itemize}

\item Plot the posterior densities $p(+|x)$ and $p(-|x)$.

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/posteriors.png}
\end{array}$
\end{center}
\end{figure}

\item How would you now decide on which label to assign? I.e. what is the decision rule that minimzes test error?

\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAP Decision Rule}

\begin{itemize}

\item It seems intuitive, that you should assign 

$$ \tilde{Y} = \begin{cases} +  &\mbox{if } P(+|x) > P(-|x)  \\ 
 - & \mbox{if } P(+|x) < P(-|x)  \end{cases}   $$

\item This is called the \textbf{Maximum A Posteriori} decision rule.

$$P(+|x) >  P(-|x) $$

\item This boils down to (after plugging in)
$$\frac{p(x|+) \times P(+)}{p(x)} > \frac{p(x|-) \times P(-)}{p(x)}$$

$$\frac{p(x|+)}{p(x|-)} > \frac{P(-)}{P(+)}$$

\item Here, we have just two values that $Y$ can take, so the decision rule is
$$P(+|x) > P(-|x) \Rightarrow  P(+|x) > 1-  P(+|x) \Rightarrow  P(+|x) > 1/2$$

\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MAP Decision Rule Minimizes Test Error}

\begin{itemize}

\item It turns out that the MAP decision rule is \emph{Bayes optimal}, it minimizes overall test error. We do not provide a formal proof, but the intuition is given

\item What is the error that we make incorporating the information contained in $x$?

$$P(error|x) = \min\{P(+|x), P(-|x) \}$$

\item The total error over all $x$ would be: 

$$ P(error) = \int_{0}^{\infty} \overbrace{p(error, x)}^{\text{joint}} dx = \int_{0}^{\infty} P(error|x) p(x) dx$$

\item  MAP decision rule minimizes this error, because for every value of $x$, we choose

$$\min\{P(+|x), P(-|x) \}$$

\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Classification Error}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Classification - Types of Errors}

\begin{itemize}

\item In regression, we would evaluate the performance of a predictive model $\hat{f}$ by studying \emph{MSE} on a validation set.

$$ MSE = \frac{1}{n} \sum_{j}^{n} (y_j - \hat{f(x_{ij})})^2$$

\item For numeric variables, we can over-estimate or underestimate. We can tweak our objective function used to estimate the coefficients to penalize over- versus underestimates.


\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Test and Training Error}

\begin{itemize}

\item We have argued that the intuitive Maximum A Posteriori Decision rule is \emph{ optimal}, i.e. it reduces the overall training error.

\item Accuracy is computed as the count of the correct assignments relative to the validation set size $N$, formally

$$  \frac{1}{N} \sum_{j=1}^N {I(\hat{Y_j}  = Y_j)}$$

where MAP says

$$ \hat{Y}  = argmax_{y \in \mathcal{C}} \hat{P}(Y=y|X) $$

\item As opposed to regression, we can better distinguish the types of errors we make.


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Two Types of Errors}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.5]{figures/type1type2error.png} 
\end{array}$
%\caption{Three dimensional linear regression visualized.}
\end{center}
\end{figure}

\textbf{Important:} the MAP decision rule minimizes the \emph{overall error rate}. But this may come at the expense of high (low) type 1 versus low (high) type 2 error rates!


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Precision, Recall and Accuracy}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/type1type2error-precision.png} 
\includegraphics[scale=.4]<2>{figures/type1type2error-recall.png} 
\includegraphics[scale=.4]<3>{figures/type1type2error-specificity.png} 
\includegraphics[scale=.4]<4>{figures/type1type2error-accuracy.png} 

\end{array}$
%\caption{Three dimensional linear regression visualized.}
\end{center}
\end{figure}
\begin{itemize}
\item<1-> $Precision = \frac{True Positives}{True Positive + False Positive}$
\item<2-> $Recall = \frac{True Positives}{True Positive + False Negatives}$
\item<3-> $Specificity = \frac{True Negative}{True Negative + False Positive}$
\item<4-> $Accuracy = \frac{True Positives + True Negative}{All Cases}$

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Precision, Recall and Accuracy}
\begin{itemize}
\item The MAP decision rule minimizes overall test error, i.e. it maximizes Accuracy $\Rightarrow$ this could however have any implications regarding the distribution of errors across false positive or false negative 

\item As with numeric prediction, one central issue is that of over-fitting the data.



\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


<<trainingmse,echo=FALSE, fig.keep ='all', include=FALSE>>=
library(data.table)
set.seed(131)
x=rnorm(150)
set.seed(132)
y=50+0.5*x-0.1*x^2+0.25*x^3+rnorm(150,mean=0,sd=1)
set.seed(133)
testset<-sample(1:150,50)
MAT <-data.frame(x,y)
xx <- seq(-2,2,length=50)
RSQ<-NULL
TEST<-NULL
for(i in seq(1,7,1)) {
fit2 <- lm(y~poly(x,i,raw=TRUE), data=MAT[-testset,])
plot(MAT[-testset,],pch=19,ylim=c(40,55))
lines(xx, predict(fit2, data.frame(x=xx)), col="red")

TMP<-data.table(cbind(MAT[testset,], yhat=predict(fit2, data.frame(x=MAT[testset,1]))))
TEST<-c(TEST,mean(TMP[,(y-yhat)^2]))

RSQ<-c(RSQ,mean(fit2$residuals^2))
}
@
<<trainingmsersq,echo=FALSE, fig.keep ='all', include=FALSE>>=
for(i in 1:length(RSQ)) {
plot(RSQ)
points(i,RSQ[i], col="red", pch=16)
lines(RSQ[1:i])
textxy(i,RSQ[i], paste("MSE =", round(RSQ[i],3),sep=""), cex=2)
}
@
<<testmsersq,echo=FALSE, fig.keep ='all', include=FALSE>>=
for(i in 1:length(RSQ)) {
plot(TEST)
points(i,TEST[i], col="blue", pch=16)
lines(TEST[1:i])
textxy(i,TEST[i], paste("MSE =", round(TEST[i],3),sep=""), cex=2)
abline(h=1, col="green")
}
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\foreach \n in {1,2,...,7} { 
\begin{frame}{Training MSE versus test MSE Evolution}
$
\begin{array}{ccc}
\text{Polynomial of order \n} & \text{Training MSE} & \text{Test MSE} \\
\includegraphics[scale=.2]{figures/knitr-trainingmse-\n.pdf}  & \includegraphics[scale=.2]{figures/knitr-trainingmsersq-\n.pdf} & \includegraphics[scale=.2]{figures/knitr-testmsersq-\n.pdf}
\end{array}$
This is fitting a \n -th order polynomial to the scatterplot on the left. As we increase the order, the \textbf{training MSE} decreases monotonically, while the \textbf{test MSE} stops decreasing after a certain point. We are \emph{overfitting} the data.
\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Overfitting...why does this happen?}

\begin{itemize}

\item In the given example, we know that the minimal attainable test MSE =1, since the irreducible error has variance 1, given $\epsilon \sim N(0,1)$.

\item Fitting ever more complicated polynomials, while ignoring the true model implies, that the estimated models are starting to \textbf{explain the noise} contained in $\epsilon$. 

\item From Econometrics: including \emph{irrelevant variables} (that is those with coefficients $\approx 0$) does not result in biased point estimates, but the resulting estimators are not \emph{efficient}, i.e. OLS is not BLUE. 

\item Since the noise is randomly drawn and thus, the noise in the training set is \textbf{independent} from the noise in the test set, the performance of the fit estimated from the data in the training set will become worse and worse

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Logistic Regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The General Classification Problem}

\begin{itemize}

\item A qualitative variable takes a value in set $\mathcal{C}$, for example $\text{Teaching Feedback} \in \{\text{Positive, Negative, Neutral}\}$, or, binary, $\text{Email} \in \{\text{Spam, No Spam}\}$.

\item We will denote $|\mathcal{C}| = c$ the number of different categories.

\item We still observe a feature vector $X$ and a responsive variable $Y$ that takes on values from the set $\mathcal{C}$

\item The MAP Decision rule says that we should assign

$$ \hat{Y}  = argmax_{y \in \mathcal{C}} \hat{P}(Y=y|X) $$

\item As we saw, in the binary case, where $c=2$, this boils down to assigning an observation to $Y=1$ if $\hat{P}(Y=1|X)>.5$


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Getting a $\hat{P}(Y=y|X)$ }


There are different ways to arrive at $\hat{P}(Y=y|X)$.

\begin{itemize}

\item In this section we will present three methods for obtaining a $\hat{P}(Y=y|X)$.

\item They belong to two distinct classes of estimators

\begin{enumerate}

\item Discriminative

\item Generative

\end{enumerate}

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Difference between Generative vs Discriminative}

The classification problem
$$ \hat{Y}  = argmax_{y \in \mathcal{C}} \hat{P}(Y=y|X) $$

A conditional probability is defined as $P(Y|X) = \frac{P(X,Y)}{P(X)}$

\begin{enumerate}

\item A \textbf{discriminative model} tries to learn the distribution of $P(Y|X)$ 

\item A \textbf{generative model} tries to learn the data generating distribution $P(X,Y)$

\end{enumerate}

We will start by discussing logistic regression, which is explicitly discriminative, we then discuss k-Nearest Neighbours as an intuitive model, that turns out to be discriminative; lastly, we discuss Naive Bayes as an example of an explicitly generative model.


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{Logistic Regression}

\begin{itemize}

\item Logistic regression takes the form

$$P(Y=c|X) = h(\beta_0 + \sum_{k=1}^{p}{\beta_k X_k})$$

\item where $h(z)$ is the sigmoid function.

$$h(z) = \frac{e^{z}}{1 + e^{z}}$$

\item Note as $z \rightarrow -\infty$, 

\item $h(z) \rightarrow 0$, $z=0, h(z)= 1/2$ and 

\item $z\rightarrow \infty$,$h(z) \rightarrow 1$.

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{Logistic Regression}
<<logisticplot, echo=FALSE, size="tiny", fig.width=2.5, fig.height=2.5>>=
par(oma = c(5,4,0,0) + 0.1,  mar = c(0,0,1,1) + 0.1) 
x= seq(-10,10,.01)
plot(cbind(x,exp(x)/(1+exp(x))),cex=.1)
@
Substituting:
$$ P(Y=1 |X ) = \frac{e^{\beta_0 + \sum_{k=1}^{p}{\beta_k X_k}}}{1 + e^{\beta_0 + \sum_{k=1}^{p}{\beta_k X_k}}}$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Logistic Regression...is linear regression in disguise}
Suppose you call $P(Y=1|X) = p(X)$, then the logistic function becomes.

$$ p(X\beta)= \frac{e^{\beta_0 + \sum_{k=1}^{p}{\beta_k X_k}}}{1 + e^{\beta_0 + \sum_{k=1}^{p}{\beta_k X_k}}} = \frac{e^{X\beta}}{1+ e^{X\beta}}$$

You can rearrange this as

$$\frac{p(X\beta)}{1-p(X\beta)} = e^{\beta_0 + \sum_{k=1}^{p}{\beta_k X_k}} = e^{X\beta}$$

This looks almost like a linear model... the LHS has an intuitive explanation. In the numerator is $P(Y=1|X)$, while the denominator is $P(Y=0|X)$. This is called the ``odds ratio''. 

Taking logs:

$$\log( \frac{p(X\beta)}{1-p(X\beta)}) = X\beta$$


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Estimating Logistic Regression using MLE}

The coefficient vector $\beta$ is unknown. We will estimate it by maximizing the likelihood function $\rightarrow$ \textbf{Maximum Likelihood}.

We assume that the individual pairs of observations are \emph{independent} from one another.

$$\mathcal{L}(\beta|X) = \prod_{i: y_i=1}{p(x_i)} \prod_{j: y_j=0}{(1-p(x_j))} $$

This is the joint likelihood of observing, out of a sample of $N$ observations, a subset $I$ with $y_i =1$ and a complementary subset $J$ with $y_j=0$:

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Estimating Logistic Regression using MLE}

We can rewrite in the binary case since $y_i \in \{0,1\}$, so the likelihood of an individual observation $y_i$ 

$$p(x_i'\beta)^{y_i} \times (1-p(x_i'\beta))^{1-y_i}$$

The joint likelihood becomes:

$$\mathcal{L}(\beta|X) = \prod_{i=1}^{n}{p(x_i'\beta)^{y_i}(1-p(x_i'\beta))^{1-y_i}} $$
 
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Rewriting the Log Likelihood Function}

Taking logs...

$$log(\mathcal{L}(\beta|X)) = \sum_{i=1}^{n}{y_i \log(p(x_i'\beta)) + (1-y_i) \log((1-p(x_i'\beta)))}$$

You want to find a vector $\beta$ that maximizes the above expression. Since the function is concave, a maximum will statisfy a first order condition:

$$\frac{\partial log(\mathcal{L}(\beta|X))}{\partial \beta} = \sum_{i=1}^{n}{(y_i - p(x_i'\beta))x_i} = 0$$

The reason why the FOC is relatively easy to derive, is because the derivative of a logistic is easy...

$$h(z) = \frac{e^{z}}{1 + e^{z}}$$

One can show that 
$$h'(z) =  h(z) (1-h(z))$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



 <<decisionboundary, include=FALSE, echo=FALSE, fig.show='hide',  message=FALSE, warning=FALSE,results="hide", size="tiny">>=
options(stringsAsFactors=FALSE)
library(data.table)
library(plyr)
library(class)
FORESTED <- data.table(read.csv("R/forested.csv"))
COMPOSITE <- data.table(read.csv("R/composite.csv"))
MODIS <- data.table(read.csv("R/modis.csv"))
setnames(MODIS, "mean", "landcover")
setnames(FORESTED, "mean", "forestcover")
DF<- join(join(FORESTED[, c("system.index", "forestcover"), with=F],
               MODIS[, c("system.index", "landcover"), with=F]), COMPOSITE)
DF[, Forested := forestcover>.8]
DF[, MODISforested := (landcover>0 & landcover<=5)]
DF[, B3:=scale(B3)]
DF[, B4:=scale(B4)]
df.xlim <- range(DF$B3)
df.ylim<- range(DF$B4)

DF.plot<-rbind(DF[Forested==TRUE][1:120],DF[Forested==FALSE][1:80])
 
plot(DF.plot[Forested==TRUE][, c("B3", "B4"), with=F], col="green", xlim=df.xlim, ylim=df.ylim)
points(DF.plot[Forested==FALSE][, c("B3", "B4"), with=F], col="red", xlim=df.xlim, ylim=df.ylim, pch=4)
glm.fit<-glm(Forested ~  B3 + B4,             data=DF.plot, family=binomial(link=logit))

##plotting the decision boundary in this case.
plot(DF.plot[Forested==TRUE][, c("B3", "B4"), with=F], col="green", xlim=df.xlim, ylim=df.ylim)
slope <- coef(glm.fit)[2]/(-coef(glm.fit)[3])
intercept <- coef(glm.fit)[1]/(-coef(glm.fit)[3]) 
abline(intercept , slope)

plot(DF.plot[Forested==TRUE][, c("B3", "B4"), with=F], col="green", xlim=df.xlim, ylim=df.ylim)
slope <- coef(glm.fit)[2]/(-coef(glm.fit)[3])
intercept <- coef(glm.fit)[1]/(-coef(glm.fit)[3]) 
abline(intercept , slope)
points(DF.plot[Forested==FALSE][, c("B3", "B4"), with=F], col="red", xlim=df.xlim, ylim=df.ylim, pch=4)

@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{A simple example: Logistic Regression}

\begin{figure}[h]
\begin{center}$
\begin{array}{c}
\includegraphics[scale=.4]<1>{figures/knitr-decisionboundary-3.pdf}
\end{array}$
\end{center}
\end{figure}

Here, we fit a logistic regression with two variables and an intercept on a dummy variable.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{A simple example: Logistic Regression}

The output in R looks as follows:

<<glmfitforested>>=
summary(glm.fit)
@


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A simple example: Logistic Regression and MAP Rule}

\begin{itemize}
\item The Maximum Likelihood is estimated using the log-odds transformation, so we have 
$$\log( \frac{p(X\beta)}{1-p(X\beta)}) = \hat{\beta}_0 + X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2$$  
\item With $\hat{\beta}_0=1.146$, $\hat{\beta}_1= -2.398$ and $\hat{\beta}_2 = -0.997$.
\item We minimize the Bayes Error rate if we follow the MAP Decision Rule!
\item MAP Decision rule. Set $y_i = 1$ if $\hat{P}(y_i=1 | x_i) > 1/2$
\item Now $\log(.5/ (1-.5)) = 0$, so the MAP decision rule translates into
$$\hat{\beta}_0 + X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 > 0$$
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{A simple example: Logistic Regression and Decision Boundary }

We have seen that the MAP Decision Rule translates to

$$\hat{\beta}_0 + X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 > 0$$

So we can rewrite this as an equation
$$ X_2 > -\frac{\hat{\beta}_0}{ \hat{\beta}_2} - X_1 \frac{\hat{\beta}_1}{ \hat{\beta}_2}$$

Which in this case is:
$$ 
X_2> \frac{1.146}{0.997} + \frac{-2.398}{0.977} X_1
$$
which naturally we can plot, since its a straight line with intercept.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{An Example with Real Data: Mortgage Applications}

\begin{itemize}

\item We start with an example where $|\mathcal{C}| = c = 2$, i.e. a binary example. For this purpose, we use data from the US Home Mortgage Disclosure Act (HMDA).

\item The HMDA requires certain financial institutions to provide data on all mortgage applications and decisions on these applications, along with some characteristics.

\item Pretty big database, in 2012, there were 7,400 institutions that reported a total of 18.7 million HMDA records.

\item We want to see which patterns predict, whether an application is $\{\text{Rejected},\text{Not Rejected}\}$. This can be expressed as a dummy variable, where

$$
Y \; = \;\begin{cases} 1 &\mbox{if } \text{Application rejected}  \\ 
0  & \mbox{if } \text{Application granted} \end{cases}
$$

\end{itemize}

Data available here : \url{https://www.ffiec.gov/hmda/}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{Summary statistics}

<<mortgagesummary, size="tiny">>=
head(MORTGAGE[,c("Denied","Leverage","Minority","ApplicantIncome","LoanAmount","Female",
                 "LoanPurpose","Occupancy"),with=F])
summary(MORTGAGE[,c("Denied","Leverage","Minority","ApplicantIncome","LoanAmount","Female",
                    "LoanPurpose","Occupancy"),with=F])
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



<<mortgagefirstglance, size='tiny', fig.width=12, fig.height=5, include=FALSE,echo=FALSE>>=
p1<-ggplot(data=MORTGAGE) + 
  geom_point(aes(x=ApplicantIncome, y=LoanAmount, colour=Denied), size=.75) + theme_bw()
p2<-ggplot(data=MORTGAGE) + geom_boxplot(aes(Denied, ApplicantIncome)) + theme_bw()
p3<-ggplot(data=MORTGAGE) + geom_boxplot(aes(Denied, LoanAmount)) + theme_bw()
grid.arrange(p1, p2, p3, ncol=3)+ theme_bw()
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Some graphs...}
\includegraphics[scale=.4]<1>{figures/knitr-mortgagefirstglance-1.png}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile,shrink]{Estimating Logistic Regression using MLE}

<<glmlogitfit, size="tiny">>=
glm.fit<-glm(Denied ~  Leverage + Minority + ApplicantIncome + Female + Occupancy + LoanPurpose, 
             data=MORTGAGE, family=binomial(link=logit))
summary(glm.fit)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Interpreting the Output...}

\begin{itemize}

\item The reported coefficients tell you what is the marginal effect of a change in some $X_i$ on the log-odds ratio. 

\item Hence, you can interpret the signs, but the coefficients are not the marginal effects in terms of probabilities.

\item The marginal effects on the probabilities are \textbf{not constant}.

\item The effect of the odds of a 1-unit increase in Leverage is
$exp(0.048) = 1.048$, meaning an odds increase by around 4.8\%.

\item Note: For small $x$, $e^x \approx 1 + x$.

\item Lets look at the range of the predicted probabilities for a more saturated model

\end{itemize}

<<predictedprobs,size="tiny", echo=TRUE, fig.width=2,fig.height=2>>=
predpr <- predict(glm.fit,type=c("response"))
summary(predpr)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<saturatedmodel,size="tiny", echo=FALSE, fig.width=2,fig.height=2, message=FALSE, warning=FALSE>>=
set.seed(12312)
training<-sample(1:nrow(MORTGAGE),10000)
set.seed(12313)
test<-sample(1:nrow(MORTGAGE),1000)
glm.fit<-glm(Denied ~  Leverage + Minority + ApplicantIncome + Female + Occupancy + LoanPurpose + 
               factor(STATE_FIPS)*ApplicantIncome + Minority*ApplicantIncome +Female:ApplicantIncome + LoanPurpose:Leverage+ Occupancy:Leverage + factor(STATE_FIPS):Leverage + CoApplicant*Leverage + Female:Leverage + Minority*Leverage+ MinorityPopulation+  Population+NumberofOwneroccupiedunits+  NumberofFamilyunits, data=MORTGAGE[training],  family=binomial(link=logit))
predpr <- predict(glm.fit,type=c("response"))
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{MAP Decision Rule and Error Types}

\begin{itemize}

\item Suppose you were to suggest to \emph{only screen bad loan applications} intensively, i.e. those whose probability of being rejected are above a threshold $\bar{c}$.

\item MAP Decision rule says that the overall error rate is minimal, in case you set $\bar{c} = 1/2$.

\item Lets see how we would do in this case.


\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{Classification and MAP Rule: How are we doing?}
MAP Decision rule says that the overall error rate is minimal, in case you set $\bar{c} = 1/2$. 
<<confusiontablemap,size="tiny", fig.width=2,fig.height=2>>=
Denied=as.character(MORTGAGE[test]$Deniedfactor)
glm.probs=predict(glm.fit,MORTGAGE[test],type="response")
glm.pred=rep("Non intense check",length(glm.probs))
glm.pred[glm.probs>.5]="Intense check"
addmargins(table(glm.pred,Denied))
@
How do we do? 

\begin{itemize}
\item Accuracy 6+864=870/1000 or almost 87\% correctly classified

\item Precision = 6/(6+7) =  46\%.

\item Recall = 6/(6+123) =  4\%.

\item Specificity = 864/871 = 99\%.
\end{itemize}

We get really high precision, because most loan applications are granted!
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{Not following MAP Rule...}
Suppose you set $\bar{c}=0.2$, out of our test sample of 1000 loan applications...
<<confusiontable,size="tiny", fig.width=2,fig.height=2>>=
Denied=as.character(MORTGAGE[test]$Deniedfactor)
glm.probs=predict(glm.fit,MORTGAGE[test],type="response")
glm.pred=rep("Non intense check",length(glm.probs))
glm.pred[glm.probs>.2]="Intense check"
addmargins(table(glm.pred,Denied))
@
How do we do? 
\begin{itemize}
\item Accuracy = (46+752) /1000 or almost 80\% correctly classified

\item Precision = 46 /(46+119) =  28\% 

\item Recall = 46/(46+83) =  36\%

\item Specificity = 752/871 = 86\%.

\end{itemize}

We are trading off true positives with true negatives. 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Visualizing Accuracy, Recall and Specificty Tradeoffs}
<<confusionplotout,size="tiny", echo=FALSE, fig.height=4, fig.width=7>>=
Denied=as.character(MORTGAGE[test]$Deniedfactor)
glm.probs=predict(glm.fit,MORTGAGE[test],type="response")

MAT<-NULL
for(cbar in seq(0.01,.76,.01)) { 
glm.pred=rep("Non intense check",length(glm.probs))
glm.pred[glm.probs>cbar]="Intense check"

TABLE<-as.matrix(addmargins(table(glm.pred,Denied)))
MAT<-rbind(MAT, data.frame(cbind(c=cbar, accuracy=(TABLE[1,1]+TABLE[2,2])/1000, sensitivity=TABLE[1,1]/TABLE[3,1],  specificity=TABLE[2,2]/TABLE[3,2])))
}
MAT<-data.table(MAT)

par(mfrow=c(1,3)) 
plot(MAT[, c("c","accuracy"),with=F], main="Accuracy") 
abline(v=.5)
plot(MAT[, c("c","sensitivity"),with=F], main="Recall (Sensitivity)")
abline(v=.5)
plot(MAT[, c("c","specificity"),with=F], main="Specificity")
abline(v=.5)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Trade off between Sensitivity and Specificity}

\begin{itemize}

\item As we increase $\bar{c}$, there is a trade off between the type 1 and type 2 errors that occur.

\item For very low $\bar{c}$, a lot of loans are assigned to be intensively checked, resulting in many false positives (loans that would not have been denied being intensively checked), but relatively few false negatives - high sensitivity and low specificity.

\item As we increase $\bar{c}$, fewer loans are intensively checked; this reduces the false positive cases but increases the false negative cases - low senitivity and high specificity.

\item Overall, since most loans are granted (87.1\%), the overall increase in accuracy as we increas $\bar{c}$ is driven by specificity.


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{ROC Curve to Visualize Trade Off between Sensitivity and Specificity}

\begin{itemize}

\item ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds.

\item It comes from communications theory and stands for ``receiver operating characteristics''.

\item It plots Specificity against Sensitivity as we vary $\bar{c}$ (without showing $\bar{c}$)

\item The ideal ROC curve is in the top left corner (100 \% specificity and 100\% sensitivity)

\item The 45 degree line is the classifier that assigns observations to classes in a random fashion (e.g. a coin toss)

\item The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). 

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile, shrink]{ROC Curve in our example}
<<roccurvelogistic,size="tiny", echo=FALSE, fig.height=3, fig.width=3>>=
roccurve <- roc(MORTGAGE[training]$Denied ~ predpr)
plot(roccurve, ylim=c(0,1))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Choice of Decision Rule depends on cost }

\begin{itemize}

\item The aim of this exercise was to highlight that choice of decision criterion, i.e. cutoff $\bar{c}$ need not be dictated by the MAP decision rule that guarantees, on average, best prediction performance.

\item The question really is, what the associated costs are for having many tue positives or true negatives.

\item Optimal choice of $\bar{c}$ would take into account the potentially different costs and may give solutions that are far away from 1/2.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Model Selection Approaches}

\begin{itemize}

\item In most machine learning applications, we are worried about overfitting.

\item Overfitting can result from incorporating information contained in features that may explain - by chance - some of the variation in the training set, but perform poorly in the validation set.

\item Shrinkage methods like Lasso can be used to remove features $X$ that do not have a lot of information content in the validation sample

\item Lasso for logistic regression would maximize Log Likelihood minus a penality: 

$$\sum_{i=1}^{n}{y_i \log(p(x_i'\beta)) + (1-y_i) \log((1-p(x_i'\beta)))} -  \lambda \sum_{j=1}^{p} |\beta_j|$$


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Spam Data}

\begin{itemize}

\item Most spam detection models use a combination of numeric features derived from the data as well as indicator and counts of word features.

\item Numeric features are measures like the number or share of all caps words, length of uninterruted punctuations, sentence length,... you name it.

\item Dummy variable or count variables of word feature counts.

\item This exercise illustrates estimating logistic regression and performing Lasso to remove the feature space.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Spam Data}

<<spammy1, echo=TRUE, cache=FALSE, message=FALSE,size="tiny">>=
email <- read.csv("R/spam.csv")

email$spam <- factor(email$spam,levels=c(0,1),labels=c("important","spam"))

#features
names(email)

## fit the full model
spammy <- glm(spam ~ ., data=email, family='binomial')

table(email$spam, email$word_freq_money>0)

plot(spammy$fit~email$spam, 
	xlab="", ylab=c("fitted probability of spam"), 
	col=c("navy","red"))
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Validation Set}
<<spammy2, echo=TRUE, cache=FALSE, message=FALSE,size="tiny",out.width='2in'>>=
leaveout <- sample(1:nrow(email), 1000) ## sample 1000 random indices
# train the model WITHOUT these observations (-index removes those obs)
spamtrain <- glm(spam ~ ., data=email[-leaveout,], family='binomial')
# get the predicted probability of spam on the left out data
pspam <- predict(spamtrain, newdata=email[leaveout,], type="response")
# plot the OOS fit
plot(pspam ~ email$spam[leaveout],
	xlab="", ylab=c("predicted probability of spam"), 
	col=c("navy","red"))
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Regularized Regression}
<<spammy3, echo=TRUE, cache=FALSE, message=FALSE,size="tiny", out.width='2in'>>=
x<-model.matrix(spam~., data=email)[,-1]

lasso.mod=glmnet(x, as.factor(email$spam),alpha=1,standardize=TRUE,family='binomial')

plot(lasso.mod, xvar="lambda")
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example: Cross Validation for Parameter Selection}
<<spammy4, echo=TRUE, cache=FALSE, message=FALSE,size="tiny", out.width='2in'>>=
cv.glmmod<-cv.glmnet(x,y=as.factor(email$spam),alpha=1,standardize=TRUE,family="binomial")

 plot(cv.glmmod)
 
 
cv.glmmod$lambda.min
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Working with Text features}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Working with text features}

\begin{itemize}

\item For short fragments, its most often useful to just dummify features

\item Word counts at the sentence level do not make much sense

\item I will show an example of logistic regression or multionomial logistic regression applied to a case of two categories categorizing short pieces of text.


\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Working with text features}
<<brexitexample, echo=TRUE, cache=FALSE, message=FALSE,size="tiny", out.width='2in'>>=
library(data.table)
library(haven)
DTA<-data.table(read_dta(file="/Users/thiemo/Dropbox/Research/Blog/immigration/data/bes_f2f_original_v3.0.dta"))
DTA[, .N, by=A1][order(N,decreasing=TRUE)][1:10]


DTA[grep("immi",A1, ignore.case=TRUE)][nchar(A1)>10][1:3]$A1


@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Defining text features for predictive exercise}
<<brexitexample2, echo=TRUE, cache=TRUE, message=FALSE,size="tiny", out.width='2in'>>=
##use document scaling method to define features
DTA[, leaveeu := "remain"]
DTA[p02==1, leaveeu := "leave"]
DTA[p02<0 |p02>2, leaveeu := NA ]
DTA[, leaveeu := as.factor(leaveeu)]
DTA[, toomanyimmigrants := as.numeric(j05==1)]
DTA[j05<0 |j05>2, toomanyimmigrants := NA ]
DTA<-DTA[!is.na(leaveeu) & !is.na(toomanyimmigrants)]    
table(DTA[ ,list(toomanyimmigrants, leaveeu)])

library(quanteda)
C<-corpus(DTA$A1)
##defining features that are informative about leaveeu decision using e.g. document scaling method
docnames(C)<-DTA$finalserialno
C.dfm<-dfm(C, tolower = TRUE, stem = TRUE, removeNumbers=TRUE,removePunct=TRUE, remove = stopwords("english"))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Single most important issue facing the UK in 2015}
<<brexitexample3, echo=TRUE, cache=TRUE, message=FALSE,size="tiny", out.width='3in'>>=
plot(C.dfm)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Bayes scoring to identify relevant features}
<<brexitexample4, echo=TRUE, cache=TRUE, message=FALSE,size="tiny", out.width='3in'>>=
##use document scaling method to define features

##using Bayes scores to identify particular word features associated with Brexit
summary(DTA$leaveeu)

ws2 <- textmodel(C.dfm, as.numeric(DTA$leaveeu), model="NB", smooth=0)

#Words associated with LEAVE 
scores<-sort(log(ws2$PwGc[1,]/ws2$PwGc[2,]),decreasing=FALSE)
scores<-scores[names(topfeatures(C.dfm, n=250))]
scores<-scores[!is.infinite(scores)]
scores<-sort(scores)

head(scores)

features<-names(c(head(scores,100),tail(scores, 100)))
features<-features[nchar(features)>3]

@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Bayes scoring to identify relevant features}
<<brexitexample5, echo=TRUE, cache=TRUE, message=FALSE,size="tiny", out.width='3in'>>=
##remove features not in the leave/ remain list
dim(C.dfm)
C.subdfm<-dfm_select(C.dfm, features=features , selection="keep")

dim(C.subdfm)
##convert DFM into a simple matrix 
MAT<-data.frame("leaveeu"=DTA$leaveeu, C.subdfm)
leaveout <- sample(1:nrow(MAT), 200) ## sample 500 random indices
# train the model WITHOUT these observations (-index removes those obs)
trained <- glm(leaveeu ~ ., data=MAT[-leaveout,], binomial(link=logit))
# get the predicted probability of spam on the left out data
glm.probs <- predict(trained, newdata=MAT[leaveout,], type="response")
# plot the OOS fit

glm.pred=rep("remain",length(glm.probs))
glm.pred[glm.probs>0.5]="leave"

table(glm.pred,MAT[leaveout,]$leaveeu)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Simple MaxEnt Classification in RTextTools}
<<maxentsample, echo=TRUE, cache=FALSE, message=FALSE,size="tiny">>=
set.seed(2016)
TRAINING<-data.table(read.csv(file="/Users/thiemo/Dropbox/Research/Matteo and Thiemo/senna/classification-tree.csv"))

PERSON<-TRAINING[objecttype=="person" & label1!=""]
PERSON$label1 <- factor(PERSON$label1)
PERSON$label1num <- as.numeric(factor(PERSON$label1))

head(PERSON[label1 =="civilian",paste(verb,objectcleanpp,sep=" ")])

@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example on how to do simple MaxEnt Classification}
<<maxentsample2, echo=TRUE, cache=FALSE, message=FALSE,size="tiny">>=

head(PERSON[label1 =="terrorist",paste(verb,objectcleanpp,sep=" ")])

@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{An Example on how to do simple MaxEnt Classification}
<<maxentsample3, echo=TRUE, cache=FALSE, message=FALSE,size="tiny">>=
library(tm)
library(RTextTools)
set.seed(30012017)
#a validation set
valid<-sample(1:nrow(PERSON), 500)
PERSON$validation<- 0
PERSON[valid]$validation<-1
PERSON<-PERSON[order(validation)]
DOC<-create_matrix(c(PERSON[,paste(objectcleanpp,sep=" ")]),language="english",
                   removeNumbers=TRUE,stemWords=TRUE,removePunctuation=TRUE,removeSparseTerms=0.9999)
DOCCONT<-create_container(DOC,PERSON$label1num, trainSize=1:1200,
                          testSize=1201:nrow(PERSON), virgin=TRUE)
MOD <- train_models(DOCCONT, algorithms=c("MAXENT"))
RES <- classify_models(DOCCONT, MOD)
analytics <- create_analytics(DOCCONT, RES)
res<-data.table(analytics@document_summary)
VALID<-cbind(PERSON[validation==1],res)

#confusion matrix
table(VALID$label1,factor(VALID$MAXENTROPY_LABEL, labels=levels(VALID$label1)))

sum(diag(3) *table(VALID$MAXENTROPY_LABEL,VALID$label1))/500
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}

